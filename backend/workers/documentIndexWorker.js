import { Worker } from 'bullmq';
import { redisConnection } from '../config/redis.js';
import { DocumentSource } from '../models/DocumentSource.js';
import { getVectorStore } from '../config/vectorStore.js';
import { prepareNotionDocumentForIndexing } from '../loaders/notionDocumentLoader.js';
import logger from '../config/logger.js';
import { connectDB } from '../config/database.js';

// M2 Indexed Memory Layer - Sparse vectors for hybrid search
import { sparseVectorManager } from '../services/search/sparseVector.js';
import { guardrailsConfig } from '../config/guardrails.js';

// M3 Compressed Memory Layer
import { createOrUpdateSummary } from '../services/memory/summarization.js';
import { processDocumentEntities } from '../services/memory/entityExtraction.js';

// Real-time events for indexing status
import { emitSyncIndexing, emitSyncPageFetched } from '../services/realtimeEvents.js';

/**
 * Process document indexing job
 * @param {Object} job - BullMQ job object
 * @returns {Promise<Object>} Indexing results
 */
async function processIndexJob(job) {
  const { workspaceId, sourceId, documentContent, operation, vectorStoreIds } = job.data;

  logger.info(
    `Processing ${operation} operation for document ${sourceId} in workspace ${workspaceId}`
  );

  try {
    if (operation === 'delete') {
      return await handleDelete(workspaceId, sourceId, vectorStoreIds);
    } else if (operation === 'add' || operation === 'update') {
      return await handleAddOrUpdate(workspaceId, sourceId, documentContent, operation);
    } else {
      throw new Error(`Unknown operation: ${operation}`);
    }
  } catch (error) {
    logger.error(`Indexing failed for document ${sourceId}:`, error);

    // Update DocumentSource with error
    const docSource = await DocumentSource.findOne({ workspaceId, sourceId });
    if (docSource) {
      await docSource.addError(error);
    }

    // Emit real-time error event
    emitSyncPageFetched(workspaceId, {
      pageId: sourceId,
      title: documentContent?.title || sourceId,
      status: 'error',
      error: error.message,
    });

    throw error;
  }
}

/**
 * Handle add or update operation
 * @param {string} workspaceId - Workspace ID
 * @param {string} sourceId - Document source ID
 * @param {Object} documentContent - Document content and metadata
 * @param {string} operation - 'add' or 'update'
 * @returns {Promise<Object>} Results
 */
async function handleAddOrUpdate(workspaceId, sourceId, documentContent, operation) {
  logger.info(
    `${operation === 'add' ? 'Adding' : 'Updating'} document ${sourceId} to vector store`
  );

  // If updating, delete old chunks first
  if (operation === 'update') {
    const docSource = await DocumentSource.findOne({ workspaceId, sourceId });
    if (docSource && docSource.vectorStoreIds && docSource.vectorStoreIds.length > 0) {
      logger.debug(`Deleting ${docSource.vectorStoreIds.length} old chunks`);
      await deleteFromVectorStore(workspaceId, sourceId, docSource.vectorStoreIds);
    }
  }

  // Prepare document chunks with semantic chunking
  // CRITICAL: Pass blocks as third parameter to enable semantic chunking
  const chunks = await prepareNotionDocumentForIndexing(
    documentContent,
    workspaceId,
    documentContent.blocks // â† Pass blocks for semantic chunking
  );
  logger.info(`Prepared ${chunks.length} chunks (semantic: ${documentContent.blocks?.length > 0})`);

  if (chunks.length === 0) {
    logger.warn(`No content to index for document ${sourceId}`);
    return { chunksCreated: 0, pointIds: [] };
  }

  // Index chunks in Qdrant (dense vectors)
  const vectorStore = await getVectorStore(chunks);
  logger.debug(`Indexed ${chunks.length} chunks in Qdrant (dense vectors)`);

  // Extract Qdrant point IDs (they're generated by Qdrant)
  // Since we don't have direct access to the point IDs, we'll use metadata to track them
  // For now, we'll store the chunk indices as a reference
  const pointIds = chunks.map((_, index) => `${sourceId}_chunk_${index}`);

  // M2 INDEXED MEMORY: Index sparse vectors for hybrid search (BM25)
  try {
    const sparseDocsToIndex = chunks.map((chunk, index) => ({
      content: chunk.pageContent,
      vectorStoreId: pointIds[index],
      documentSourceId: sourceId,
      title: documentContent.title || 'Untitled',
      contentHash: chunk.metadata?.contentHash,
    }));

    await sparseVectorManager.batchIndexDocuments(workspaceId, sparseDocsToIndex);
    logger.info(`Indexed ${chunks.length} sparse vectors for hybrid search`, {
      service: 'document-index',
      workspaceId,
      sourceId,
    });

    // Update inverted index if feature flag is enabled (for optimized BM25 search)
    const sparseConfig = guardrailsConfig.retrieval?.sparseSearch || {};
    if (sparseConfig.useInvertedIndex) {
      try {
        // Get the sparse vectors we just indexed to update inverted index
        const { SparseVector } = await import('../services/search/sparseVector.js');
        const sparseVectors = await SparseVector.find({
          workspaceId,
          vectorStoreId: { $in: pointIds },
        })
          .select('vectorStoreId vector')
          .lean();

        if (sparseVectors.length > 0) {
          await sparseVectorManager.batchUpdateInvertedIndex(workspaceId, sparseVectors);
          logger.info(`Updated inverted index for ${sparseVectors.length} documents`, {
            service: 'document-index',
            workspaceId,
            sourceId,
          });
        }
      } catch (invertedError) {
        // Non-critical: optimized search will fall back to full scan
        logger.warn(`Inverted index update failed for ${sourceId}:`, {
          service: 'document-index',
          error: invertedError.message,
        });
      }
    }
  } catch (sparseError) {
    // Non-critical: hybrid search will fall back to semantic-only
    logger.warn(`Sparse vector indexing failed for ${sourceId}:`, {
      service: 'document-index',
      error: sparseError.message,
    });
  }

  // Update DocumentSource in database
  const docSource = await DocumentSource.findOne({ workspaceId, sourceId });
  if (docSource) {
    await docSource.markAsSynced(pointIds, chunks.length);
    logger.info(`Updated DocumentSource for ${sourceId}: ${chunks.length} chunks indexed`);

    // Emit real-time indexing status
    emitSyncIndexing(workspaceId, {
      documentsIndexed: 1,
      totalDocuments: 1,
      currentDocument: documentContent.title || sourceId,
    });

    // Emit page fetched with success status
    emitSyncPageFetched(workspaceId, {
      pageId: sourceId,
      title: documentContent.title || 'Untitled',
      status: 'success',
      chunksCreated: chunks.length,
    });
  } else {
    logger.warn(`DocumentSource not found for ${sourceId}, this should not happen`);
  }

  // M3 COMPRESSED MEMORY: Generate summary and extract entities
  // Run in background to not block indexing
  processM3Memory(workspaceId, docSource?._id, sourceId, documentContent).catch((err) => {
    logger.warn(`M3 memory processing failed for ${sourceId}:`, {
      service: 'document-index',
      error: err.message,
    });
  });

  return {
    chunksCreated: chunks.length,
    pointIds,
  };
}

/**
 * Process M3 Compressed Memory (summarization + entity extraction)
 * Runs asynchronously after indexing to not block the main flow
 * @param {string} workspaceId - Workspace ID
 * @param {string} documentSourceId - DocumentSource ID
 * @param {string} sourceId - Source document ID
 * @param {Object} documentContent - Document content and metadata
 */
async function processM3Memory(workspaceId, documentSourceId, sourceId, documentContent) {
  const title = documentContent.title || 'Untitled';
  const content = documentContent.content || '';

  // Skip if content is too short for meaningful M3 processing
  if (content.length < 200) {
    logger.debug(`Skipping M3 processing for ${sourceId} - content too short`);
    return;
  }

  const startTime = Date.now();

  try {
    // Generate document summary
    logger.info(`Generating M3 summary for ${sourceId}`, { service: 'document-index' });
    const summary = await createOrUpdateSummary({
      workspaceId,
      documentSourceId,
      sourceId,
      title,
      content,
    });

    // Extract entities
    logger.info(`Extracting M3 entities for ${sourceId}`, { service: 'document-index' });
    const entities = await processDocumentEntities({
      workspaceId,
      documentSourceId,
      sourceId,
      title,
      content,
    });

    // Link entities to summary
    if (summary && entities.length > 0) {
      summary.entityIds = entities.map((e) => e._id);
      await summary.save();
    }

    logger.info(`M3 memory processing complete for ${sourceId}`, {
      service: 'document-index',
      summaryLength: summary?.summary?.length || 0,
      entitiesCount: entities.length,
      processingTimeMs: Date.now() - startTime,
    });
  } catch (error) {
    logger.error(`M3 memory processing failed for ${sourceId}:`, {
      service: 'document-index',
      error: error.message,
      processingTimeMs: Date.now() - startTime,
    });
    // Don't throw - M3 processing is non-critical
  }
}

/**
 * Handle delete operation
 * @param {string} workspaceId - Workspace ID
 * @param {string} sourceId - Document source ID
 * @param {Array} vectorStoreIds - Vector store point IDs to delete
 * @returns {Promise<Object>} Results
 */
async function handleDelete(workspaceId, sourceId, vectorStoreIds) {
  logger.info(`Deleting document ${sourceId} from vector store`);

  // Delete from Qdrant using metadata filter (dense vectors)
  await deleteFromVectorStore(workspaceId, sourceId, vectorStoreIds);

  // M2 INDEXED MEMORY: Delete sparse vectors for hybrid search
  try {
    await deleteSparseVectors(workspaceId, sourceId, vectorStoreIds);
    logger.info(`Deleted sparse vectors for ${sourceId}`, { service: 'document-index' });
  } catch (sparseError) {
    logger.warn(`Sparse vector deletion failed for ${sourceId}:`, {
      service: 'document-index',
      error: sparseError.message,
    });
  }

  // Update DocumentSource in database
  const docSource = await DocumentSource.findOne({ workspaceId, sourceId });
  if (docSource) {
    await docSource.markAsDeleted();
    logger.info(`Marked DocumentSource ${sourceId} as deleted`);
  }

  return {
    deleted: true,
    sourceId,
  };
}

/**
 * Delete sparse vectors from MongoDB
 * M2 INDEXED MEMORY: Remove sparse vectors when document is deleted
 * @param {string} workspaceId - Workspace ID
 * @param {string} sourceId - Document source ID
 * @param {Array} vectorStoreIds - Vector store point IDs to delete
 * @returns {Promise<void>}
 */
async function deleteSparseVectors(workspaceId, sourceId, vectorStoreIds) {
  try {
    const { SparseVector } = await import('../services/search/sparseVector.js');

    // Delete by vectorStoreIds if available
    if (vectorStoreIds && vectorStoreIds.length > 0) {
      await SparseVector.deleteMany({
        workspaceId,
        vectorStoreId: { $in: vectorStoreIds },
      });

      // Also remove from inverted index if feature flag is enabled
      const sparseConfig = guardrailsConfig.retrieval?.sparseSearch || {};
      if (sparseConfig.useInvertedIndex) {
        try {
          for (const vectorStoreId of vectorStoreIds) {
            await sparseVectorManager.removeFromInvertedIndex(workspaceId, vectorStoreId);
          }
          logger.debug(`Removed ${vectorStoreIds.length} documents from inverted index`, {
            service: 'document-index',
            workspaceId,
          });
        } catch (invertedError) {
          logger.warn(`Failed to remove from inverted index:`, {
            service: 'document-index',
            error: invertedError.message,
          });
        }
      }
    } else {
      // Fallback: delete by sourceId pattern
      await SparseVector.deleteMany({
        workspaceId,
        vectorStoreId: { $regex: `^${sourceId}_chunk_` },
      });
    }

    logger.debug(`Deleted sparse vectors for document ${sourceId}`, {
      service: 'document-index',
      workspaceId,
    });
  } catch (error) {
    logger.error(`Failed to delete sparse vectors: ${error.message}`);
    throw error;
  }
}

/**
 * Delete document chunks from Qdrant vector store
 * Uses metadata filter to delete all chunks for a specific document
 * @param {string} workspaceId - Workspace ID
 * @param {string} sourceId - Document source ID
 * @param {Array} vectorStoreIds - Vector store point IDs (for reference)
 * @returns {Promise<void>}
 */
async function deleteFromVectorStore(workspaceId, sourceId, vectorStoreIds) {
  try {
    // Since LangChain's Qdrant integration doesn't expose delete by filter directly,
    // we need to use the Qdrant client directly
    const { QdrantClient } = await import('@qdrant/js-client-rest');
    const qdrantUrl = process.env.QDRANT_URL || 'http://localhost:6333';
    const collectionName = process.env.QDRANT_COLLECTION_NAME || 'langchain-rag';

    const client = new QdrantClient({ url: qdrantUrl });

    // Delete points using metadata filter
    await client.delete(collectionName, {
      filter: {
        must: [
          {
            key: 'metadata.workspaceId',
            match: { value: workspaceId },
          },
          {
            key: 'metadata.sourceId',
            match: { value: sourceId },
          },
        ],
      },
    });

    logger.info(`Deleted chunks for document ${sourceId} from Qdrant`);
  } catch (error) {
    logger.error(`Failed to delete from vector store: ${error.message}`);
    // Don't throw - mark as best effort
  }
}

// Create worker with extended lock duration for large documents
export const documentIndexWorker = new Worker('documentIndex', processIndexJob, {
  connection: redisConnection,
  concurrency: 20, // Process 20 documents concurrently (optimized for bulk)
  lockDuration: 300000, // Hold lock for 5 minutes (handles large docs with 1000+ chunks)
  lockRenewTime: 120000, // Renew lock every 2 minutes
});

documentIndexWorker.on('completed', (job, result) => {
  logger.info(`Index job ${job.id} completed:`, result);
});

documentIndexWorker.on('failed', (job, err) => {
  logger.error(`Index job ${job.id} failed:`, err);
});

documentIndexWorker.on('error', (err) => {
  logger.error('Document index worker error:', err);
});

// Connect to database before starting worker
(async () => {
  await connectDB();
  logger.info('Document index worker started');
})();

export default documentIndexWorker;
